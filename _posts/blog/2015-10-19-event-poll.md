---
layout: post
title: 谈谈高性能服务开发
description: 简单说一说现代高性能服务器的设计方法
category: blog
---

## 谈谈高性能服务开发

我先说一个现象。

比如《构建高性能web站点》这本书上有一章节说了这样一个例子，apache配置成prefork工作模式下，用ab去压测，测试结果是qps为5479，进程94个，进程间切换18166次，单进程内存开销14M。结果不甚理想对吧。

然后换成lighttpd，对于静态页面请求，使用单进程单线程模型，测试结果qps为13323，进程间切换数字才61，而单进程内存开销更是低到6704K。

apache的prefork模式的好处是适合于没有线程安全库，需要避免线程兼容性问题的系统。在系统有要求需要将每个请求相互独立的场景下适用，这样若一个请求出现问题就不会影响到其他请求。如果要处理潜在的请求高峰，做法就是把进程数量调大，同时又不能太大，以免使用的内存超出物理内存的大小，这是prefork模式的一个缺点，当然另外的缺点就是上边说的现象了，由于采用的阻塞模型，所以必须靠多进程来增加任务的处理能力，进程数量一旦增大，切换的开销也随之增大，消耗了大量的cpu资源，导致性能上不去，qps就很低了。

prefork模式是使用阻塞模型时的常见做法，如今的互联网应用动不动就qps达百万，如果用阻塞模型的程序来为百万并发的系统提供服务的话，可以想象，把百万并发的负载均衡到这些程序上，这服务器的数量得需要多少？不但费用成本超高，管理成本也很高，服务器数量越大就越难管。所以为了降低成本，就出现了像lighttpd这样的可以做到多路I/O复用的非阻塞应用，开销少服务能力强，只用少量的服务器就可以扛住百万并发的访问，老板省钱，程序员们管理上也轻松多了。目前这方面最著名的是在业界被广泛使用的nginx。

这些提供交互服务的程序最常见的行为就是读和写，比如读/写文件，读/写数据库，读/写网络连接，等等。读和写是属于cpu的操作指令，操作的对象是磁盘和网卡这些相对慢速的设备，cpu的速度要远远高于后者。

当有用户发出网络请求的时候，程序需要读取请求并处理以便提供服务，在此读取的过程分为阻塞读和非阻塞读。为什么会有这两种形式呢？用户发来的数据我们是希望一下读完的，但是由于硬件设备的速度差异，在这里慢速的网络传输导致了用户的请求数据传达到程序端能够读取到，相对cpu来说需要一个较长的时间，cpu为了读到这些数据，要么等在那里，要么轮询查看数据是否到位，这里等待的状态就是阻塞读，轮询查看就是非阻塞读。

总之cpu都是早早就准备好等着数据到来的。在阻塞式下，数据未准备好就需要等待，让出cpu资源给别的线程，I/O无法在此线程被其他用户的请求所复用。非阻塞式下，就是循环去查看I/O的状态，如果数据未准备好，虽然可以转去暂时处理其他I/O，但是待处理的I/O数目很多的话，如果没有一个可靠的机制支撑，这么大的数目不管状态如何每次都要轮询一遍，处理顺序该怎样安排也很不确定，那么很可能一个可读的I/O要等好久，这便影响了服务吞吐量，或者所有I/O都没有数据cpu空转，这也白白浪费了计算资源，可见这相比prefork模式没有什么优势。此时我们需要的是一种I/O状态的通知机制，可以根据通知得到可用的I/O去处理，不必扫描所有I/O，做到最有效的处理，因此就出现了select、poll、epoll等这种基于事件触发式的接口，它们配合非阻塞I/O，使多路复用成为可能。

在linux系统中，select、poll、epoll这些事件触发接口的出现算是一次革命。系统调用read、write、accept的默认行为就是阻塞式，这种阻塞的等待将会使本线程释放cpu时间片让给其他线程，所以在prefork模式下，每个线程都在频繁的阻塞并频繁释放处理器资源。但是架构已定，如此频繁的切换不可避免，效能提升空间已经十分有限。而多路I/O复用，可以做到最大限度的避免cpu等待，使cpu资源得到充分的有效利用。因此，通过使用事件触发接口对并发架构进行改进，创造出了目前高效的事件驱动架构。lighttpd单进程单线程模型比apache prefork模型处理能力强的原因就是采用了事件驱动架构，使单线程全速运转不再有休眠。

所谓事件驱动架构，就是利用操作系统提供的异步通知机制，由单线程就可以做到对多个I/O句柄进行读写。每个I/O就是一个事件发生源，当事件产生时，会有多个事件收集器来收集并且分发事件，然后会有多个事件处理器来消费这些事件。

> 作为事件发生源的I/O，我们常用到的有比如socket，pipe，各种磁盘文件的描述符，都可以用select，poll，epoll来管理。

很多服务器采用的所谓事件驱动往往局限在TCP连接建立、关闭事件上，一个连接建立以后，在其关闭之前的所有操作都不再是事件驱动，这时会退化成按顺序执行每个操作的批处理模式，这样每个请求在连接建立后都始终占有着线程资源，直到连接关闭才会把资源让给其他IO。这段时间可能会非常长，超过1分钟都有可能，而且这段时间资源都被一个连接占用并没有意义，整个事件消费进程只是在等待某个条件而已，这造成了服务器资源的极大浪费，影响了系统可以处理的并发连接数。这种设计往往把一个进程或线程作为事件消费者，当一个请求产生的事件被消费时，直到这个请求处理结束时资源都将被这一个请求所占用。

那么如何改进以便达到事件驱动的真正效果呢？在nginx的设计中，它不使用进程或线程来作为事件消费者，而是把事件收集、分发、消费，全都模块化，事件收集、分发模块来占用进程资源，在分发事件时调用消费模块使用当前占用的进程资源。为了高效，不出现事件消费者耗时过长的问题，在设计上控制事件消费者不能有阻塞行为，像进程转变为休眠状态或等待状态，是一定要避免的。因此这也加大了事件消费程序开发者的编程难度。

控制消费模块不阻塞，我们需要做到事件消费的多阶段处理，设法把请求的处理过程按照事件的触发方式划分为多个阶段，每个阶段都可以由事件收集模块来触发。也就是说，当一个事件被分发到事件消费者中进行处理时，事件消费者处理完这个事件只相当于处理完1个请求的某个阶段，若要处理下个阶段，只能等待内核通知，即当下一次事件出现时，事件收集模块将会获取到通知，分发器分发，再继续调用事件消费者处理请求。每个阶段中的事件消费者都不清楚本次完整的操作什么时候会完成，只能被动地等待下一次事件的通知。所以，多路复用和多阶段触发是相辅相成的，这种设计配合事件驱动架构，将会使得每个进程都能全力运转，不会或者尽量少地出现进程休眠状况，极大地提高了服务的效率。

--------

## 结语

请求的多阶段处理将会提高网络性能、降低请求的时延，在与事件驱动架构配合工作后，就可以使得服务器同时处理十万甚至百万级别的并发连接了。

常常有人把linux系统下的事件触发说成是异步，认为epoll是异步编程的核心，其实在linux系统下epoll不算真正的异步，AIO才是，windows下的IOCP也是。至于epoll为什么不算异步，有人说是因为epoll只做I／O通知不做数据从内核到用户空间的拷贝，并且epoll本身的事件触发是轮询，这我过段时间会续写一部分来解答。我自己对epoll的原理还有好几个疑问，比如epoll的触发是不是内核利用网卡的中断之后回调？还是说epoll触发是内核线程的轮询？或者用户态的轮询？还是说在数据不频繁的时候用网卡中断，数据很频繁的时候转变成内核线程轮询？这些都是很细节的问题，等有时间再来深入研究。
